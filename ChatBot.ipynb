{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ChatBot Mental Health"],"metadata":{"id":"aYhqHpp_xFAs"}},{"cell_type":"markdown","source":["# 1. Connect Dengan Drive"],"metadata":{"id":"cHvasaAhrj3H"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"IKPwGNaAjUWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#open data\n","import json\n","with open('/content/intentsindo.json') as file:\n","  data = json.load(file)\n","\n","print(data)"],"metadata":{"id":"xBQi-HRa3vwh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Download Library"],"metadata":{"id":"8rpItpFtsXRu"}},{"cell_type":"code","source":["import nltk\n","import pickle\n","import numpy as np\n","import tensorflow as tf\n","import random\n","!pip install tensorflow\n","!pip install keras\n","!pip install keras-models\n","!pip install nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"metadata":{"id":"umAXGCksa3kC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from nltk.stem import WordNetLemmatizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout\n","from tensorflow.keras.optimizers import SGD\n","\n","from keras.models import load_model\n","\n","# mendefinisikan variabel\n","words=[]\n","classes = []\n","documents = []\n","ignore_words = ['?', '!']\n","\n","#membaca file json dan menyimpan ke dalam variabel intents\n","data_file = open(\"/content/intentsindo.json\").read()\n","intents = json.loads(data_file)"],"metadata":{"id":"4QDT__-DYnPW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Data Pre-Processing\n","\n","* Tokenization\n","* Membah dokumen dari corpus\n","* Menambahkan Ke kelas"],"metadata":{"id":"JzlD9_7psrXV"}},{"cell_type":"code","source":["for intent in intents['intents']:\n","    if 'patterns' not in intent:\n","        intent['patterns'] = []\n","    for pattern in intent['patterns']:\n","        # tokenisasi kata\n","        w = nltk.word_tokenize(pattern)\n","        words.extend(w)\n","        # menambah dokumen ke dalam corpus\n","        documents.append((w, intent['tag']))\n","    # menambah ke dalam list kelas\n","    if intent['tag'] not in classes:\n","        classes.append(intent['tag'])\n","print(\"words\", words)\n","print(\"documents\",documents)\n","print(\"classes\",classes)"],"metadata":{"id":"kXneel00q67t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# membuat lematization\n","lemmatizer = WordNetLemmatizer()\n","words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n","words = sorted(list(set(words)))\n","\n","# mengurutkan kelas\n","classes = sorted(list(set(classes)))\n","\n","# documents \n","print(len(documents), \"documents\")\n","\n","# intents sebagai kelas\n","print(len(classes), \"classes\", classes)\n","\n","# vocab\n","print(len(words), \"unique lemmatized words\", words)\n","\n","# membuat file pickle \n","pickle.dump(words,open('texts.pkl','wb')) \n","pickle.dump(classes,open('label.pkl','wb'))"],"metadata":{"id":"apYVpR6QZRzH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Data Training"],"metadata":{"id":"KAVgzemmtljR"}},{"cell_type":"code","source":["training = []\n","\n","output_empty = [0]* len(classes)\n","\n","for doc in documents:\n","  #instal bag of word\n","  bag = []\n","\n","  #list untuk patterns\n","  patterns_words = doc[0]\n","\n","  patterns_words = [lemmatizer.lemmatize(word.lower()) for word in patterns_words]\n","\n","  for w in words:\n","    bag.append(1) if w in patterns_words else bag.append(0)\n","\n","  output_row = list(output_empty)\n","  output_row[classes.index(doc[1])] = 1\n","  training.append([bag, output_row])\n","\n","random.shuffle(training)\n","training = np.array(training)\n","\n","train_x = list(training[:, 0])\n","train_y = list(training[:, 1])\n","\n","print(\"Data Telah di Training\")"],"metadata":{"id":"FpvS6Iv3jJku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_x))\n","print(len(train_y))"],"metadata":{"id":"O1_b3VpLqoPn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#contoh data yang telah dirubah menjadi vektor numerik dalam bentuk array\n","train_x"],"metadata":{"id":"2xWvsqnI0gry"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Modeling Neural Network"],"metadata":{"id":"kS87m7Nx1zzm"}},{"cell_type":"code","source":["from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.layers import Reshape\n","import matplotlib.pyplot as plt\n","#ini library dari tensorflow untuk agar kecepatan melatih model dapat disesuaikan\n","\n","model = Sequential()\n","model.add(Dense(261, input_shape=(len(train_x[0]),), activation='relu'))\n","\n","#menambah lapisan input layer #shape karna data berupa array\n","model.add(Dropout(0.5))\n","#menambahkan lapisan dropout agar tidak overfitting\n","model.add(Dense(64, activation='relu'))\n","#hidden layer\n","model.add(Dropout(0.5))\n","model.add(Dense(len(train_y[0]), activation='softmax'))\n","#output layer\n","\n","rmsprop = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.001, decay=1e-6)\n","model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n","#algoritma deep learning agar model tidak loss function\n","#loss fubction untuk multiclass klasification\n","\n","#memasang dan menyimpan model\n","hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n","model.save('chatbot.h5', hist) \n","\n","plt.plot(hist.history['accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train'], loc='upper left')\n","plt.show()"],"metadata":{"id":"XQEYAhqX1xST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"lJBYTLbhUV8W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Repeat -> mengulangi langkah agar dapat merespon terus menerus"],"metadata":{"id":"VLlPkA8DezoF"}},{"cell_type":"code","source":["#simpan dalam file model\n","from tensorflow.keras.models import load_model\n","model = load_model('/content/chatbot.h5')\n","intents = json.loads(open(\"/content/intentsindo.json\").read())\n","words = pickle.load(open('/content/texts.pkl','rb'))\n","classes = pickle.load(open('/content/label.pkl','rb'))"],"metadata":{"id":"sxFFHn0ll_z1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(np.array(train_x).shape)"],"metadata":{"id":"njyjust8oYth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words"],"metadata":{"id":"N66dhGFEdUxZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","# memprediksi kelas dari kalimat yang ditraining sebelumnya\n","def clean_up_sentence(sentence):\n","\n","    # pisahkan data dan dalam bentuk array\n","    sentence_words = nltk.word_tokenize(sentence)\n","    \n","    # stem each word - create short form for word\n","    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n","    return sentence_words\n","\n","# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n","\n","def bow(sentence, words, show_details=True, return_bag=False):\n","    # tokenize the pattern\n","    sentence_words = clean_up_sentence(sentence)\n","\n","    # bag of words - matrix of N words, vocabulary matrix\n","    bag = [0] * len(words)\n","    for s in sentence_words:\n","        for i, w in enumerate(words):\n","            if w == s:\n","                # assign 1 if current word is in the vocabulary position\n","                bag[i] = 1\n","                if show_details:\n","                    print(\"found in bag: %s\" % w)\n","        return np.array(bag)\n","\n","# memprediksi kelas dari kalimat yang ditraining sebelumnya\n","max_sequence_length = 406\n","def predict_class(sentence, model, words, classes, max_sequence_length):\n","    p = bow(sentence, words, show_details=False, return_bag=True)\n","    p = np.array([p])\n","    padded_input_data = pad_sequences(p, maxlen=max_sequence_length, padding='post', truncating='post')\n","    return_list = []\n","    res = model.predict(padded_input_data)[0]\n","    error = 0.25\n","    results = [[i,r] for i,r in enumerate(res) if r>error]\n","\n","    # sort by strength of probability\n","    results.sort(key=lambda x: x[1], reverse=True)\n","    for r in results:\n","        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n","    return return_list\n"],"metadata":{"id":"ujcS8eo-mV_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(predict_class( \"stres\",  model, words, classes, max_sequence_length))"],"metadata":{"id":"jFZ4hgqCpPpp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# berfungsi untuk mendapatkan respons dari model\n","\n","def getResponse(ints, intents_json):\n","    tag = ints[0]['intent']\n","    list_of_intents = intents_json['intents']\n","    result = None  # initialize result variable\n","    for i in list_of_intents:\n","        if i['tag'] == tag:\n","            result = random.choice(i['responses'])\n","            break\n","    if result is None:  # handle case where no matching intent is found\n","        result = \"I'm sorry, I didn't understand what you said.\"\n","    return result\n","\n","def chatbot_response(text):\n","    # memprediksi kelas intent dari kalimat yang diinput\n","    ints = predict_class(text, model, words, classes, max_sequence_length)\n","\n","    # mencari respon terbaik berdasarkan kelas intent\n","    res = getResponse(ints, intents)\n","\n","    return res"],"metadata":{"id":"lPv8ZYEVmkCW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# berfungsi untuk mendapatkan respons dari model\n","\n","def getResponse(ints, intents_json):\n","  try:\n","    tag = ints[0]['intent']\n","    list_of_intents = intents_json['intents']\n","    result = None  # initialize result variable\n","    for i in list_of_intents:\n","        if i['tag'] == tag:\n","            result = random.choice(i['responses'])\n","            break\n","    if result is None:  # handle case where no matching intent is found\n","        result = \"Maaf, tampaknya saya tidak mengerti maksud anda. Saya akan mempelajari maksud anda.\"\n","  except IndexError:\n","        result = \"Maaf, tampaknya saya tidak mengerti maksud anda. Saya akan mempelajari maksud anda\"\n","  return result\n"],"metadata":{"id":"Bh4hQjHwfRNA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bot akan berakhir jika mengetik end\n","\n","def start_chat():\n","    print(\"Bot: This is Sophie! Your Personal Assistant.\\n\\n\")\n","    while True:\n","        inp = str(input()).lower()\n","        if inp.lower()==\"end\":\n","            break\n","        if inp.lower()== '' or inp.lower()== '*':\n","            print('Please re-phrase your query!')\n","            print(\"-\"*50)\n","        else:\n","            print(f\"Bot: {chatbot_response(inp)}\"+'\\n')\n","            print(\"-\"*50)"],"metadata":{"id":"9BEBktu8moc_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 7. Coba BotChat"],"metadata":{"id":"rbHewF6ChoMZ"}},{"cell_type":"code","source":["# start the chat bot\n","start_chat()"],"metadata":{"id":"6uSR_VEGmrxJ"},"execution_count":null,"outputs":[]}]}